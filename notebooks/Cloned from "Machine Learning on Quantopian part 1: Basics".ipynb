{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Machine Learning on Quantopian\n",
    "Recently, Quantopian’s Chief Investment Officer, Jonathan Larkin, shared an industry insider’s overview of the [professional quant equity workflow](http://blog.quantopian.com/a-professional-quant-equity-workflow/). This workflow is comprised of distinct stages including: (1) Universe Definition, (2) Alpha Discovery, (3) Alpha Combination, (4) Portfolio Construction and (5) Trading.\n",
    "\n",
    "This Notebook focuses on stage 3: Alpha Combination. At this stage, Machine Learning is an intuitive choice as we have abstracted the problem to such a degree that it is now a classic classification (or regression) problem which ML is very good at solving and coming up with an alpha combination that is predictive.\n",
    "\n",
    "As you will see, there is a lot of code here setting up a factor library and some data wrangling to get everything into shape. The details of this part are perhaps not quite as interesting so feel free to skip directly to [\"Training our ML pipeline\"](#training) where we have everything in place to train and test our classifier.\n",
    "\n",
    "## Overview\n",
    "1. Define trading universe to use ([Q500US and Q1500US](https://www.quantopian.com/posts/the-q500us-and-q1500us)).\n",
    "2. Define alphas (implemented in [Pipeline](https://www.quantopian.com/tutorials/pipeline)).\n",
    "3. Run pipeline.\n",
    "4. Split into train and test set.\n",
    "5. Preprocess data (rank alphas, subsample, align alphas with future returns, impute, scale).\n",
    "6. Train Machine Learning classifier ([AdaBoost from Scikit-Learn](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)).\n",
    "7. Evaluate Machine Learning classifier on test set.\n",
    "\n",
    "Note that one important limitation is that we only train and test on static (i.e. fixed-in-time) data. Thus, you can not directly do the same in an algorithm. In principle, this is possible and will be the next step but it makes sense to first focus on just the ML in a more direct way to get a good intuition about the workflow and how to develop a competitive ML pipeline.\n",
    "\n",
    "### Disclaimer\n",
    "This workflow is still a bit rough around the edges. We are working on improving it and adding better educational materials. This serves as a sneak-peek for the curious and adventurous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantopian.research import run_pipeline\n",
    "from quantopian.pipeline import Pipeline\n",
    "from quantopian.pipeline.factors import Latest\n",
    "from quantopian.pipeline.data.builtin import USEquityPricing\n",
    "from quantopian.pipeline.data import morningstar\n",
    "from quantopian.pipeline.factors import CustomFactor, SimpleMovingAverage, AverageDollarVolume, Returns, RSI\n",
    "from quantopian.pipeline.classifiers.morningstar import Sector\n",
    "from quantopian.pipeline.filters import Q500US, Q1500US\n",
    "from quantopian.pipeline.data.quandl import fred_usdontd156n as libor\n",
    "from quantopian.pipeline.data.zacks import EarningsSurprises\n",
    "\n",
    "import talib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import alphalens as al\n",
    "import pyfolio as pf\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, decomposition, ensemble, preprocessing, isotonic, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of some commonly used factors\n",
    "The factors below are a small collection of commonly used alphas that were coded by Gil Wassermann. I will post a separate Notebook with the full collection and more descriptions of them. Ultimately we will put these into a library you can just import to avoid the wall of text. If you want to understand more about pipeline, read the [tutorial](https://www.quantopian.com/tutorials/pipeline).\n",
    "\n",
    "Also note the `Earnings_Quality` alpha which uses [Zacks Earnings Surprises](https://www.quantopian.com/data/zacks/earnings_surprises), a [new source from our partners](https://www.quantopian.com/data).\n",
    "\n",
    "The details of these factors are not the focus of this Notebook so feel free to just [skip](#universe) this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = morningstar.balance_sheet\n",
    "cfs = morningstar.cash_flow_statement\n",
    "is_ = morningstar.income_statement\n",
    "or_ = morningstar.operation_ratios\n",
    "er = morningstar.earnings_report\n",
    "v = morningstar.valuation\n",
    "vr = morningstar.valuation_ratios\n",
    "\n",
    "\n",
    "def make_factors():\n",
    "    def Asset_Growth_3M():\n",
    "        return Returns(inputs=[bs.total_assets], window_length=63)\n",
    "\n",
    "    def Asset_To_Equity_Ratio():\n",
    "        return bs.total_assets.latest / bs.common_stock_equity.latest\n",
    "\n",
    "    def Capex_To_Cashflows():\n",
    "        return (cfs.capital_expenditure.latest * 4.) / \\\n",
    "            (cfs.free_cash_flow.latest * 4.)\n",
    "        \n",
    "    def EBITDA_Yield():\n",
    "        return (is_.ebitda.latest * 4.) / \\\n",
    "            USEquityPricing.close.latest        \n",
    "\n",
    "    def EBIT_To_Assets():\n",
    "        return (is_.ebit.latest * 4.) / \\\n",
    "            bs.total_assets.latest\n",
    "        \n",
    "    def Earnings_Quality():\n",
    "        return morningstar.cash_flow_statement.operating_cash_flow.latest / \\\n",
    "               EarningsSurprises.eps_act.latest\n",
    "        \n",
    "    def Return_On_Total_Invest_Capital():\n",
    "        return or_.roic.latest\n",
    "    \n",
    "    class Mean_Reversion_1M(CustomFactor):\n",
    "        inputs = [Returns(window_length=21)]\n",
    "        window_length = 252\n",
    "\n",
    "        def compute(self, today, assets, out, monthly_rets):\n",
    "            out[:] = (monthly_rets[-1] - np.nanmean(monthly_rets, axis=0)) / \\\n",
    "                np.nanstd(monthly_rets, axis=0)\n",
    "                \n",
    "    class MACD_Signal_10d(CustomFactor):\n",
    "        inputs = [USEquityPricing.close]\n",
    "        window_length = 60\n",
    "\n",
    "        def compute(self, today, assets, out, close):\n",
    "\n",
    "            sig_lines = []\n",
    "\n",
    "            for col in close.T:\n",
    "                # get signal line only\n",
    "                try:\n",
    "                    _, signal_line, _ = talib.MACD(col, fastperiod=12,\n",
    "                                                   slowperiod=26, signalperiod=10)\n",
    "                    sig_lines.append(signal_line[-1])\n",
    "                # if error calculating, return NaN\n",
    "                except:\n",
    "                    sig_lines.append(np.nan)\n",
    "            out[:] = sig_lines \n",
    "            \n",
    "    class Moneyflow_Volume_5d(CustomFactor):\n",
    "        inputs = [USEquityPricing.close, USEquityPricing.volume]\n",
    "        window_length = 5\n",
    "\n",
    "        def compute(self, today, assets, out, close, volume):\n",
    "\n",
    "            mfvs = []\n",
    "\n",
    "            for col_c, col_v in zip(close.T, volume.T):\n",
    "\n",
    "                # denominator\n",
    "                denominator = np.dot(col_c, col_v)\n",
    "\n",
    "                # numerator\n",
    "                numerator = 0.\n",
    "                for n, price in enumerate(col_c.tolist()):\n",
    "                    if price > col_c[n - 1]:\n",
    "                        numerator += price * col_v[n]\n",
    "                    else:\n",
    "                        numerator -= price * col_v[n]\n",
    "\n",
    "                mfvs.append(numerator / denominator)\n",
    "            out[:] = mfvs  \n",
    "            \n",
    "           \n",
    "    def Net_Income_Margin():\n",
    "        return or_.net_margin.latest           \n",
    "\n",
    "    def Operating_Cashflows_To_Assets():\n",
    "        return (cfs.operating_cash_flow.latest * 4.) / \\\n",
    "            bs.total_assets.latest\n",
    "\n",
    "    def Price_Momentum_3M():\n",
    "        return Returns(window_length=63)\n",
    "    \n",
    "    class Price_Oscillator(CustomFactor):\n",
    "        inputs = [USEquityPricing.close]\n",
    "        window_length = 252\n",
    "\n",
    "        def compute(self, today, assets, out, close):\n",
    "            four_week_period = close[-20:]\n",
    "            out[:] = (np.nanmean(four_week_period, axis=0) /\n",
    "                      np.nanmean(close, axis=0)) - 1.\n",
    "    \n",
    "    def Returns_39W():\n",
    "        return Returns(window_length=215)\n",
    "    \n",
    "    class Trendline(CustomFactor):\n",
    "        inputs = [USEquityPricing.close]\n",
    "        window_length = 252\n",
    "\n",
    "        # using MLE for speed\n",
    "        def compute(self, today, assets, out, close):\n",
    "\n",
    "            # prepare X matrix (x_is - x_bar)\n",
    "            X = range(self.window_length)\n",
    "            X_bar = np.nanmean(X)\n",
    "            X_vector = X - X_bar\n",
    "            X_matrix = np.tile(X_vector, (len(close.T), 1)).T\n",
    "\n",
    "            # prepare Y matrix (y_is - y_bar)\n",
    "            Y_bar = np.nanmean(close, axis=0)\n",
    "            Y_bars = np.tile(Y_bar, (self.window_length, 1))\n",
    "            Y_matrix = close - Y_bars\n",
    "\n",
    "            # prepare variance of X\n",
    "            X_var = np.nanvar(X)\n",
    "\n",
    "            # multiply X matrix an Y matrix and sum (dot product)\n",
    "            # then divide by variance of X\n",
    "            # this gives the MLE of Beta\n",
    "            out[:] = (np.sum((X_matrix * Y_matrix), axis=0) / X_var) / \\\n",
    "                (self.window_length)\n",
    "        \n",
    "    class Vol_3M(CustomFactor):\n",
    "        inputs = [Returns(window_length=2)]\n",
    "        window_length = 63\n",
    "\n",
    "        def compute(self, today, assets, out, rets):\n",
    "            out[:] = np.nanstd(rets, axis=0)\n",
    "            \n",
    "    def Working_Capital_To_Assets():\n",
    "        return bs.working_capital.latest / bs.total_assets.latest\n",
    "        \n",
    "    all_factors = {\n",
    "        'Asset Growth 3M': Asset_Growth_3M,\n",
    "        'Asset to Equity Ratio': Asset_To_Equity_Ratio,\n",
    "        'Capex to Cashflows': Capex_To_Cashflows,\n",
    "        'EBIT to Assets': EBIT_To_Assets,\n",
    "        'EBITDA Yield': EBITDA_Yield,        \n",
    "        'Earnings Quality': Earnings_Quality,\n",
    "        'MACD Signal Line': MACD_Signal_10d,\n",
    "        'Mean Reversion 1M': Mean_Reversion_1M,\n",
    "        'Moneyflow Volume 5D': Moneyflow_Volume_5d,\n",
    "        'Net Income Margin': Net_Income_Margin,        \n",
    "        'Operating Cashflows to Assets': Operating_Cashflows_To_Assets,\n",
    "        'Price Momentum 3M': Price_Momentum_3M,\n",
    "        'Price Oscillator': Price_Oscillator,\n",
    "        'Return on Invest Capital': Return_On_Total_Invest_Capital,\n",
    "        '39 Week Returns': Returns_39W,\n",
    "        'Trendline': Trendline,\n",
    "        'Vol 3M': Vol_3M,\n",
    "        'Working Capital to Assets': Working_Capital_To_Assets,        \n",
    "    }        \n",
    "    \n",
    "    return all_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a></a></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define universe and select factors to use\n",
    "We will screen our universe using the new [Q1500US](https://www.quantopian.com/posts/the-q500us-and-q1500us) and hand-pick a few alphas from the list above. We encourage you to play around with the factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe = Q1500US()\n",
    "\n",
    "factors = make_factors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and build the pipeline\n",
    "Next we have to build the pipeline. In addition to the factors defined above, we need the forward returns we want to predict. In this Notebook we will predict 5-day returns and train our model on daily data. You can also subsample the data to e.g. weekly to not have overlapping return periods but we omit this here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fwd_days = 5 # number of days to compute returns over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_history_pipeline(factors, universe, n_fwd_days=5):\n",
    "    # Call .rank() on all factors and mask out the universe\n",
    "    factor_ranks = {name: f().rank(mask=universe) for name, f in factors.iteritems()}\n",
    "    # Get cumulative returns over last n_fwd_days days. We will later shift these.\n",
    "    factor_ranks['Returns'] = Returns(inputs=[USEquityPricing.open],\n",
    "                                      mask=universe, window_length=n_fwd_days)\n",
    "    \n",
    "    pipe = Pipeline(screen=universe, columns=factor_ranks)\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_pipe = make_history_pipeline(factors, universe, n_fwd_days=n_fwd_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_timer = time()\n",
    "start = pd.Timestamp(\"2016-03-06\")\n",
    "end = pd.Timestamp(\"2016-09-14\")\n",
    "results = run_pipeline(history_pipe, start_date=start, end_date=end)\n",
    "results.index.names = ['date', 'security']\n",
    "end_timer = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Time to run pipeline %.2f secs\" % (end_timer - start_timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, running pipeline gives us factors for every day and every security, ranked relative to each other. We assume that the order of individual factors might carry some weak predictive power on future returns. The question then becomes: how can we combine these weakly predictive factors in a clever way to get a single mega-alpha which is hopefully more predictive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an important milestone. We have our ranked factor values on each day for each stock. Ranking is not absolutely necessary but has several benefits:\n",
    "\n",
    "* it increases robustness to outliers,\n",
    "* we mostly care about the relative ordering rather than the absolute values.\n",
    "\n",
    "Also note the `Returns` column. These are the values we want to predict given the factor ranks.\n",
    "\n",
    "Next, we are doing some additional transformations to our data:\n",
    "\n",
    "1. Shift factor ranks to align with future returns `n_fwd_days` days in the future.\n",
    "2. Find the top and bottom 30 percentile stocks by their returns. Essentially, we only care about relative movement of stocks. If we later short stocks that go down and long stocks going up relative to each other, it doesn't matter if e.g. all stocks are going down in absolute terms. Moverover, we are ignoring stocks that did not move that much (i.e. 30th to 70th percentile) to only train the classifier on those that provided strong signal. \n",
    "3. We also binarize the returns by their percentile to turn our ML problem into a classification one.\n",
    "\n",
    "`shift_mask_data()` is a utility function that does all of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_mask_data(X, Y, upper_percentile=70, lower_percentile=30, n_fwd_days=1):\n",
    "    # Shift X to match factors at t to returns at t+n_fwd_days (we want to predict future returns after all)\n",
    "    shifted_X = np.roll(X, n_fwd_days+1, axis=0)\n",
    "    \n",
    "    # Slice off rolled elements\n",
    "    X = shifted_X[n_fwd_days+1:]\n",
    "    Y = Y[n_fwd_days+1:]\n",
    "    \n",
    "    n_time, n_stocks, n_factors = X.shape\n",
    "    \n",
    "    # Look for biggest up and down movers\n",
    "    upper = np.nanpercentile(Y, upper_percentile, axis=1)[:, np.newaxis]\n",
    "    lower = np.nanpercentile(Y, lower_percentile, axis=1)[:, np.newaxis]\n",
    "  \n",
    "    upper_mask = (Y >= upper)\n",
    "    lower_mask = (Y <= lower)\n",
    "    \n",
    "    mask = upper_mask | lower_mask # This also drops nans\n",
    "    mask = mask.flatten()\n",
    "    \n",
    "    # Only try to predict whether a stock moved up/down relative to other stocks\n",
    "    Y_binary = np.zeros(n_time * n_stocks)\n",
    "    Y_binary[upper_mask.flatten()] = 1\n",
    "    Y_binary[lower_mask.flatten()] = -1\n",
    "    \n",
    "    # Flatten X\n",
    "    X = X.reshape((n_time * n_stocks, n_factors))\n",
    "\n",
    "    # Drop stocks that did not move much (i.e. are in the 30th to 70th percentile)\n",
    "    X = X[mask]\n",
    "    Y_binary = Y_binary[mask]\n",
    "    \n",
    "    return X, Y_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have our helper function to align our data properly we pass our factor ranks to it. You might wonder why we have to do the `swapaxes` thing below rather than just using `pandas` logic. The reason is that this way we can use the same `shift_mask_data()` function inside of a factor where we do not have access to a Pandas `DataFrame`. More on that in a future notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Massage data to be in the form expected by shift_mask_data()\n",
    "results_wo_returns = results.copy()\n",
    "returns = results_wo_returns.pop('Returns')\n",
    "Y = returns.unstack().values\n",
    "X = results_wo_returns.to_panel() \n",
    "X = X.swapaxes(2, 0).swapaxes(0, 1).values # (factors, time, stocks) -> (time, stocks, factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_wo_returns.index = results_wo_returns.index.set_levels(results_wo_returns.index.get_level_values(1).map(lambda x: x.symbol), 1, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_wo_returns.index = results_wo_returns.index.set_levels(results_wo_returns.index.get_level_values(0).map(lambda x: x.date), 0, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_wo_returns.sample(10).sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = (returns > 0.).to_frame()\n",
    "tmp.index = tmp.index.set_levels(tmp.index.get_level_values(1).map(lambda x: x.symbol), 1)\n",
    "tmp.columns = ['5-day forward returns > 0']\n",
    "tmp.sample(10).sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split our data into training (80%) and test (20%). This is common practice: our classifier will try to fit the training set as well as possible but it does not tell us how well it would perform on unseen data. Because we are dealing with time-series data we split along the time-dimension to only test on future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train_size_perc = 0.8\n",
    "n_time, n_stocks, n_factors = X.shape\n",
    "train_size = np.int16(np.round(train_size_perc * n_time))\n",
    "X_train, Y_train = X[:train_size, ...], Y[:train_size]\n",
    "X_test, Y_test = X[(train_size+n_fwd_days):, ...], Y[(train_size+n_fwd_days):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can only exclude stocks that did not move by a lot (i.e. 30th to 70th percentile) during training, we keep all stocks in our test set and just binarize according to the median. This avoids look-ahead bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_shift, Y_train_shift = shift_mask_data(X_train, Y_train, n_fwd_days=n_fwd_days)\n",
    "X_test_shift, Y_test_shift = shift_mask_data(X_test, Y_test, n_fwd_days=n_fwd_days, \n",
    "                                             lower_percentile=50, \n",
    "                                             upper_percentile=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_shift.shape, X_test_shift.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a></a></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our ML pipeline\n",
    "Before training our classifier, several preprocessing steps are advisable. The first one imputes nan values with the factor mean to get clean training data, the second scales our factor ranks to be between [0, 1).\n",
    "\n",
    "For training we are using the [AdaBoost classifier](https://en.wikipedia.org/wiki/AdaBoost) which automatically determines the most relevant features (factors) and tries to find a non-linear combination of features to maximize predictiveness while still being robust. In essence, AdaBoost trains an ensemble of weak classifiers (decision trees in this case) sequentially. Each subsequent weak classifier takes into account the samples (or data points) already classified by the previous weak classifiers. It then focuses on the samples misclassified by the previous weak classifiers and tries to get those correctly. With each new weak classifier you get more fine-grained in your decision function and correctly classify some previously misclassified samples. For prediction, you simply average the answer of all weak classifiers to get a single strong classifier.\n",
    "\n",
    "Of course, this is just an example and you can let your creativity and skill roam freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_timer = time()\n",
    "\n",
    "# Train classifier\n",
    "imputer = preprocessing.Imputer()\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "clf = ensemble.AdaBoostClassifier(n_estimators=150) # n_estimators controls how many weak classifiers are fi\n",
    "\n",
    "X_train_trans = imputer.fit_transform(X_train_shift)\n",
    "X_train_trans = scaler.fit_transform(X_train_trans)\n",
    "clf.fit(X_train_trans, Y_train_shift)\n",
    "\n",
    "end_timer = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Time to train full ML pipline: %0.2f secs\" % (end_timer - start_timer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, training a modern ML classifier does not have to be very compute intensive. Scikit-learn is heavily optimized so the full process only takes less than 10 seconds. Of course, things like deep-learning (which is currently not available on Quantopian), might take a bit longer, but these models are also trained on data sets much much bigger than this (a famous subset of the ImageNet data set is 138 GB).\n",
    "\n",
    "This means that the current bottlenecks are retrieving the data from pipeline (RAM and i/o), not lack of GPU or parallel processing support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf.predict(X_train_trans)\n",
    "print('Accuracy on train set = {:.2f}%'.format(metrics.accuracy_score(Y_train_shift, Y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier does reasonably well on the data we trained it on, but the real test is on hold-out data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise*: It is also common to run cross-validation on the training data and tweak the parameters based on that score, testing should only be done rarely. Try coding a [sklearn pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) with [K-Fold cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our ML classifier\n",
    "To evaluate our ML classifer on the test set, we have to transform our test data in the same way as our traning data. Note that we are only calling the `.transform()` method here which does not use any information from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test data\n",
    "X_test_trans = imputer.transform(X_test_shift)\n",
    "X_test_trans = scaler.transform(X_test_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all this work, we can finally test our classifier. We can predict binary labels but also get probability estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict!\n",
    "Y_pred = clf.predict(X_test_trans)\n",
    "\n",
    "Y_pred_prob = clf.predict_proba(X_test_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Predictions:', Y_pred\n",
    "print 'Probabilities of class == 1:', Y_pred_prob[:, 1] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to evaluate the performance of our classifier. The simplest and most intuitive one is certainly the accuracy (50% is chance due to our median split). On Kaggle competitions, you will also often find the log-loss being used. This punishes you for being wrong *and* confident in your answer. See  [the Kaggle description](https://www.kaggle.com/wiki/LogarithmicLoss) for more motivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy on test set = {:.2f}%'.format(metrics.accuracy_score(Y_test_shift, Y_pred) * 100))\n",
    "print('Log-loss = {:.5f}'.format(metrics.log_loss(Y_test_shift, Y_pred_prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we're at chance on this data set, alas. But perhaps you can do better?\n",
    "\n",
    "We can also examine which factors the classifier identified as most predictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.Series(clf.feature_importances_, index=results_wo_returns.columns)\n",
    "feature_importances.sort(ascending=False)\n",
    "ax = feature_importances.plot(kind='bar')\n",
    "ax.set(ylabel='Importance (Gini Coefficient)', title='Feature importances');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise*: Use [partial dependence plots](http://scikit-learn.org/stable/auto_examples/ensemble/plot_partial_dependence.html) to get an understanding of how factor rankings are used to predict future returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go from here\n",
    "Several knobs can be tweaked to boost performance:\n",
    "\n",
    "* Add existing factors from the collection above to the data set.\n",
    "* Come up with new factors \n",
    "  * Use [`alphalens`](https://www.quantopian.com/posts/alphalens-a-new-tool-for-analyzing-alpha-factors) to evaluate an alpha for its predictive power.\n",
    "  * Look for [novel data sources from our partners](https://www.quantopian.com/data).\n",
    "  * Look at the [101 Alpha's Project](https://www.quantopian.com/posts/the-101-alphas-project).\n",
    "* Improve preprocessing of the ML pipeline\n",
    "  * Is 70/30 the best split? \n",
    "  * Should we not binarize the returns and do regression? \n",
    "  * Can we add Sector information in some way?\n",
    "* Experiment with [feature selection](http://scikit-learn.org/stable/modules/feature_selection.html).\n",
    "  * PCA\n",
    "  * ICA\n",
    "  * etc.\n",
    "* Tweak hyper-parameters of `AdaBoostClassifier`.\n",
    "  * [Use cross-validation to find optimal parameters](http://scikit-learn.org/stable/modules/grid_search.html).\n",
    "* Try [different classifiers](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) of combinations of classifiers.\n",
    "\n",
    "## Machine Learning competition\n",
    "If you have something you think works well, post it in this thread. Make sure to test over the same time-period as I have here to keep things comparable. In a month from now, we can test on new data that has aggregated since then and determine who built the best ML pipeline. If there is demand, we might turn this into a proper ML contest.\n",
    "\n",
    "## Machine Learning resources\n",
    "If you look for information on how to get started with ML, here are a few resources:\n",
    "\n",
    "* [Scikit-learn resources](http://scikit-learn.org/stable/presentations.html)\n",
    "* [Learning scikit-learn: Machine Learning in Python](https://www.amazon.com/dp/1783281936)\n",
    "* [Pattern Recognition and Machine Learning](https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738)\n",
    "\n",
    "## How to put this into an algorithm\n",
    "As mentioned above, this is not immediately usable in an algorithm. For one thing, there is no `run_pipeline()` in the backtest IDE. It turns out to be rather simple to take the code above and put it into a pipeline `CustomFactor()` where the ML model would automatically get retrained and make predictions. You would then long the `1` predictions and short the `-1` predictions, apply some weighting (e.g. inverse variance) and execute orders. More on these next steps in the future.\n",
    "\n",
    "## Credits\n",
    "* Content created by James Christopher and Thomas Wiecki\n",
    "* Thanks to Sheng Wang for ideas and inspiration.\n",
    "* Jonathan Larkin, Jess Stauth, Delaney Granizo-Mackenzie, and Jean Bredeche for helpful comments on an earlier draft."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
