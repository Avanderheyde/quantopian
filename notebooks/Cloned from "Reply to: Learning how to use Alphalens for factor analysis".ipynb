{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 101 Alphas \\#2 with Parameter Optimization\n",
    "From the paper [101 Formulaic Alphas](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2701346)\n",
    "\n",
    "\\\\( (-1 * correlation(rank(delta(log(volume), 2)), rank(((close - open) / open)), 6)) \\\\)\n",
    "\n",
    "This factor returns a negative value if the change in volume is highly correlated with intraday return. In other words, if volume increases (decreases) by a lot on days where the intraday return is high (low), this factor is negative. \n",
    "\n",
    "I am postulating that the idea behind this factor is that large moves with heavy volume are liquidity demanding trades (ideally by uninformed traders). Traders providing liquidity in these instances would demand a premium/discount to take the other side to compensate for the risk that they may be trading with an informed trader or the risk of being stuck with an inventory too large. Note, this is quite the opposite of how technical analysis generally looks at the volume/price relationships (although I am oversimplifying a bit with this statement). \n",
    "\n",
    "My in-sample data for this runs from 2003 to 2012. However, it should be noted that this paper was published in 2015. Therefore, any out-of-sample testing should be done on data after 2015, once the researcher gets to that stage. 2012 to 2015 could possibly be used as sort of a cross-validation set to tune hyper parameters if any kind of machine learning is used to tweak the factor.\n",
    "\n",
    "### Parameter Optimization\n",
    "In this notebook, I will perform a bit of parameter optimization, in part to see what the best parameters or for performance. However, I am more interested in seeing how sensitive the performance of the factor is to changes in the input parameters. If performance is super sensitive to small changes in the inputs, then I would give a higher likelihood that the researchers overfit this factor. \n",
    "\n",
    "To keep things simple for the moment, I will only adjust the correlation lookback window in the optimization. In the future, I may work on tweaking other parameters if I can find an efficient workflow for doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typical imports for use with Pipeline\n",
    "from quantopian.pipeline import Pipeline, CustomFactor\n",
    "from quantopian.research import run_pipeline\n",
    "from quantopian.pipeline.data.builtin import USEquityPricing\n",
    "from quantopian.pipeline.data import Fundamentals  \n",
    "from quantopian.pipeline.classifiers.fundamentals import Sector \n",
    "from quantopian.pipeline.filters import QTradableStocksUS, Q500US\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import alphalens as al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  VolumeChange(CustomFactor):\n",
    "    \"\"\"Factor returning the change in log volume as compared\n",
    "    to (window_length - 1) days ago. Essentially, this is the\n",
    "    percent change in volume.\"\"\"\n",
    "    inputs = [USEquityPricing.volume]\n",
    "    window_length = 3\n",
    "    window_safe=True\n",
    "  \n",
    "    def compute(self, today, asset_ids, out, volume):\n",
    "        out[:] = np.log(volume[-1]) - np.log(volume[-3])\n",
    "        \n",
    "class IntradayReturn(CustomFactor):\n",
    "    \"\"\"Factor returning the return from today's open to \n",
    "    today's close\"\"\"\n",
    "    inputs = [USEquityPricing.open, USEquityPricing.close]\n",
    "    window_length = 1\n",
    "    window_safe=True  \n",
    "    def compute(self, today, asset_ids, out, open_, close):\n",
    "        out[:] = close / open_ - 1\n",
    "\n",
    "def make_alpha_2(mask, window_length=6):\n",
    "    \"\"\"Construct factor returning the negative of the rank correlation over the \n",
    "    past 'window_length' days between the intraday return and the VolumeChange.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    mask: Filter\n",
    "        Filter representing what assets get included in factor computation.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Factor\n",
    "    \n",
    "    Notes: This is a measure of whether returns are correlated with volume. It is\n",
    "    negative when volume is stronger on up moves and light on down moves. It is \n",
    "    positive when volume is stronger on down moves and lighter on up moves.\n",
    "        \"\"\"\n",
    "    class Alpha2(CustomFactor):\n",
    "#         inputs = [VolumeChange().rank(), IntradayReturn().rank()]\n",
    "#         window_length = 6\n",
    "\n",
    "        def compute(self, today, asset_ids, out, volume_change, intraday_return):\n",
    "            volume_change_df = pd.DataFrame(volume_change)\n",
    "            intraday_return_df = pd.DataFrame(intraday_return)\n",
    "            out[:]=-volume_change_df.corrwith(intraday_return_df)\n",
    "        \n",
    "    return Alpha2(mask=mask, \n",
    "                  inputs = [VolumeChange(mask=mask).rank(), \n",
    "                            IntradayReturn(mask=mask).rank()],\n",
    "                  window_length=window_length\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline(corr_param_range):\n",
    "    base_universe = QTradableStocksUS()\n",
    "#     base_universe = Fundamentals.symbol.latest.element_of(['GS', 'AAPL', 'XOM'])\n",
    "    closed_end_funds = Fundamentals.share_class_description.latest.startswith('CE')\n",
    "    universe = base_universe & ~closed_end_funds\n",
    "    \n",
    "    factor_dict = {}\n",
    "    for i in corr_param_range:\n",
    "        factor_dict['alpha_2_{}'.format(i)] = make_alpha_2(universe, i)\n",
    "\n",
    "    factor_dict['sector_code'] = Sector(mask=universe)\n",
    "    \n",
    "    return Pipeline(columns=factor_dict, screen=universe)\n",
    "\n",
    "start_date = '2003-01-01' \n",
    "end_date = '2012-12-31'\n",
    "# end_date = '2003-01-10'\n",
    "corr_param_range = [4,6,8,10,12,14,16,18,20]\n",
    "\n",
    "result = run_pipeline(make_pipeline(corr_param_range), start_date, end_date, chunksize=504)  \n",
    "col_order = []\n",
    "\n",
    "# Reorder Columns\n",
    "for i in corr_param_range:\n",
    "    col_order.append('alpha_2_{}'.format(i))\n",
    "col_order.append('sector_code')\n",
    "result = result[col_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get `factor_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_al_prices(result, periods=(1,5,21)):\n",
    "    assets = result.index.levels[1].unique()\n",
    "    start_date = result.index.get_level_values(0)[0] \n",
    "    end_date = result.index.get_level_values(0)[-1]  + max(periods) * pd.tseries.offsets.BDay()\n",
    "    pricing = get_pricing(assets, start_date, end_date, fields=\"open_price\")\n",
    "    return pricing \n",
    "\n",
    "def get_factor_data(result, \n",
    "                    factor_col, \n",
    "                    prices,\n",
    "                    forward_returns,\n",
    "                    quantiles=5,\n",
    "                    bins=None, \n",
    "                    groupby=None, \n",
    "                    binning_by_group=False,\n",
    "                    groupby_labels=None,\n",
    "                    max_loss=0.35):\n",
    "\n",
    "#     pricing = get_al_prices(result, periods)\n",
    "    \n",
    "#     factor_data = al.utils.get_clean_factor_and_forward_returns(factor=result[factor_col],\n",
    "#                                                                 prices=pricing,\n",
    "#                                                                 groupby=groupby,\n",
    "#                                                                 binning_by_group=binning_by_group,\n",
    "#                                                                 groupby_labels=groupby_labels,\n",
    "#                                                                 quantiles=quantiles,\n",
    "#                                                                 bins=bins,\n",
    "#                                                                 periods=periods,\n",
    "#                                                                 max_loss=max_loss)\n",
    "    \n",
    "    factor_data = al.utils.get_clean_factor(result[factor_col], \n",
    "                                            forward_returns,\n",
    "                                            groupby=groupby,\n",
    "                                            binning_by_group=binning_by_group,\n",
    "                                            groupby_labels=groupby_labels,\n",
    "                                            quantiles=quantiles,\n",
    "                                            bins=bins,\n",
    "                                            max_loss=max_loss)\n",
    "    \n",
    "    return factor_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize by Correlation Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods=(1,3,5,7,10,12,15,20)\n",
    "prices = get_al_prices(result, periods)\n",
    "forward_returns = al.utils.compute_forward_returns(result[result.columns[0]], prices, periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factor_data={}\n",
    "ic_dict={}\n",
    "for factor_col in result.columns:\n",
    "    if factor_col != 'sector_code':\n",
    "        print \"-\"*30 + \"\\nGetting Factor Data for '{}'\".format(factor_col)\n",
    "        factor_data = get_factor_data(result, \n",
    "                                      factor_col, \n",
    "                                      prices,\n",
    "                                      forward_returns)\n",
    "        print \"-\"*30 + \"\\nCalculating ICs for '{}'\".format(factor_col)\n",
    "        ic_dict[factor_col] = al.performance.mean_information_coefficient(factor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_df = pd.DataFrame.from_dict(ic_dict)[col_order[:-1]]\n",
    "ic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_df.loc['5D'].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_df.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(ic_df, annot=True, cmap='RdBu', vmin=-.01, vmax=.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tearsheet on Original Params\n",
    "Correlation_window = 6 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prices, factor_data = get_factor_data(result, 'alpha_2')\n",
    "factor_data = get_factor_data(result, \n",
    "                              ['alpha_2_6'], \n",
    "                              prices,\n",
    "                              forward_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al.tears.create_full_tear_sheet(factor_data, long_short=True, group_neutral=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tearsheet on Optimized Params\n",
    "Correlation_window = 16 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_data = get_factor_data(result, \n",
    "                              ['alpha_2_16'], \n",
    "                              prices,\n",
    "                              forward_returns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al.tears.create_full_tear_sheet(factor_data.drop(['3D', '7D', '12D', '15D', '20D'], axis=1), \n",
    "                                long_short=True, group_neutral=False )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
