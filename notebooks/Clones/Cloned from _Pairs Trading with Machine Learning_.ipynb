{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairs Trading with Machine Learning\n",
    "Jonathan Larkin\n",
    "\n",
    "August, 2017\n",
    "\n",
    "In developing a Pairs Trading strategy, finding valid, eligible pairs which exhibit unconditional mean-reverting behavior is of critical importance. This notebook walks through an example implementation of finding eligible pairs. We show how popular algorithms from Machine Learning can help us navigate a very high-dimensional seach space to find tradeable pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from statsmodels.tsa.stattools import coint\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from quantopian.pipeline.data import morningstar\n",
    "from quantopian.pipeline.filters.morningstar import Q500US, Q1500US, Q3000US\n",
    "from quantopian.pipeline import Pipeline\n",
    "from quantopian.research import run_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Numpy: %s \" % np.__version__\n",
    "print \"Pandas: %s \" % pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_date = \"2016-12-31\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Universe\n",
    "We start by specifying that we will constrain our search for pairs to the a large and liquid single stock universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe = Q1500US()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Data\n",
    "In addition to pricing, let's use some fundamental and industry classification data. When we look for pairs (or model anything in quantitative finance), it is generally good to have an \"economic prior\", as this helps mitigate overfitting. I often see Quantopian users create strategies with a fixed set of pairs that they have likely chosen by some fundamental rationale (\"KO and PEP should be related becuase...\"). A purely fundamental approach is a fine way to search for pairs, however breadth will likely be low. As discussed in [The Foundation of Algo Success](https://blog.quantopian.com/the-foundation-of-algo-success/), you can maximize Sharpe by having high breadth (high number of bets). With `N` stocks in the universe, there are `N*(N-1)/2` pair-wise relationships. However, if we do a brute-force search over these, we will likely end up with many spurious results. As such, let's narrow down the search space in a reasonable way. In this study, I start with the following priors:\n",
    "\n",
    "- Stocks that share loadings to common factors (defined below) in the past should be related in the future.\n",
    "- Stocks of similar market caps should be related in the future.\n",
    "- We should exclude stocks in the industry group \"Conglomerates\" (industry code 31055). Morningstar analysts classify stocks into industry groups primarily based on similarity in revenue lines. \"Conglomerates\" is a catch-all industry. As described in the [Morningstar Global Equity\n",
    "Classification Structure manual](http://corporate.morningstar.com/us/documents/methodologydocuments/methodologypapers/equityclassmethodology.pdf): \"If the company has more than three sources of revenue and\n",
    "income and there is no clear dominant revenue and income stream, the company\n",
    "is assigned to the Conglomerates industry.\" We should not expect these stocks to be good members of any pairs in the future. This turns out to have zero impact on the Q500 and removes only 1 stock from the Q1500, but I left this idea in for didactic purposes.\n",
    "- Creditworthiness in an important feature in future company performance. It's difficult to find credit spread data and map the reference entity to the appropriate equity security. There is a model, colloquially called the [Merton Model](http://www.investopedia.com/terms/m/mertonmodel.asp), however, which takes a contingent claims approach to modeling the capital structure of the firm. The output is an implied probability of default. Morningstar analysts calculate this for us and the field is called `financial_health_grade`. A full description of this field is in the [help docs](https://www.quantopian.com/help/fundamentals#asset-classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    columns= {\n",
    "        'Market Cap': morningstar.valuation.market_cap.latest.quantiles(5),\n",
    "        'Industry': morningstar.asset_classification.morningstar_industry_group_code.latest,\n",
    "        'Financial Health': morningstar.asset_classification.financial_health_grade.latest\n",
    "    },\n",
    "    screen=universe\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run_pipeline(pipe, study_date, study_date)\n",
    "res.index = res.index.droplevel(0)  # drop the single date from the multi-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print res.shape\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stocks in Industry \"Conglomerates\"\n",
    "res = res[res['Industry']!=31055]\n",
    "print res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stocks without a Financial Health grade\n",
    "res = res[res['Financial Health']!= None]\n",
    "print res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the categorical data with numerical scores per the docs\n",
    "res['Financial Health'] = res['Financial Health'].astype('object')\n",
    "health_dict = {u'A': 0.1,\n",
    "               u'B': 0.3,\n",
    "               u'C': 0.7,\n",
    "               u'D': 0.9,\n",
    "               u'F': 1.0}\n",
    "res = res.replace({'Financial Health': health_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Horizon\n",
    "We are going to work with a daily return horizon in this strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pricing = get_pricing(\n",
    "    symbols=res.index,\n",
    "    fields='close_price',\n",
    "    start_date=pd.Timestamp(study_date) - pd.DateOffset(months=24),\n",
    "    end_date=pd.Timestamp(study_date)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pricing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pricing.pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns[symbols(['AAPL'])].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can only work with stocks that have the full return series\n",
    "returns = returns.iloc[1:,:].dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print returns.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Candidate Pairs\n",
    "Given the pricing data and the fundamental and industry/sector data, we will first classify stocks into clusters and then, within clusters, looks for strong mean-reverting pair relationships.\n",
    "\n",
    "The first hypothesis above is that \"Stocks that share loadings to common factors in the past should be related in the future\". Common factors are things like sector/industry membership and widely known ranking schemes like momentum and value. We could specify the common factors *a priori* to well known factors, or alternatively, we could let the data speak for itself. In this post we take the latter approach. We use PCA to reduce the dimensionality of the returns data and extract the historical latent common factor loadings for each stock. For a nice visual introduction to what PCA is doing, take a look [here](http://setosa.io/ev/principal-component-analysis/) (thanks to Gus Gordon for pointing out this site).\n",
    "\n",
    "We will take these features, add in the fundamental features, and then use the `DBSCAN` **unsupervised** [clustering algorithm](http://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html#dbscan) which is available in [`scikit-learn`](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html). Thanks to Thomas Wiecki for pointing me to this specific clustering technique and helping with implementation. Initially I looked at using `KMeans` but `DBSCAN` has advantages in this use case, specifically\n",
    "\n",
    "- `DBSCAN` does not cluster *all* stocks; it leaves out stocks which do not neatly fit into a cluster;\n",
    "- relatedly, you do not need to specify the number of clusters.\n",
    "\n",
    "The clustering algorithm will give us sensible *candidate* pairs. We will need to do some validation in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Decomposition and DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PRIN_COMPONENTS = 50\n",
    "pca = PCA(n_components=N_PRIN_COMPONENTS)\n",
    "pca.fit(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reduced data now with the first `N_PRIN_COMPONENTS` principal component loadings. Let's add some fundamental values as well to make the model more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack(\n",
    "    (pca.components_.T,\n",
    "     res['Market Cap'][returns.columns].values[:, np.newaxis],\n",
    "     res['Financial Health'][returns.columns].values[:, np.newaxis])\n",
    ")\n",
    "\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessing.StandardScaler().fit_transform(X)\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DBSCAN(eps=1.9, min_samples=3)\n",
    "print clf\n",
    "\n",
    "clf.fit(X)\n",
    "labels = clf.labels_\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print \"\\nClusters discovered: %d\" % n_clusters_\n",
    "\n",
    "clustered = clf.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the initial dimensionality of the search was\n",
    "ticker_count = len(returns.columns)\n",
    "print \"Total pairs possible in universe: %d \" % (ticker_count*(ticker_count-1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_series = pd.Series(index=returns.columns, data=clustered.flatten())\n",
    "clustered_series_all = pd.Series(index=returns.columns, data=clustered.flatten())\n",
    "clustered_series = clustered_series[clustered_series != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_SIZE_LIMIT = 9999\n",
    "counts = clustered_series.value_counts()\n",
    "ticker_count_reduced = counts[(counts>1) & (counts<=CLUSTER_SIZE_LIMIT)]\n",
    "print \"Clusters formed: %d\" % len(ticker_count_reduced)\n",
    "print \"Pairs to evaluate: %d\" % (ticker_count_reduced*(ticker_count_reduced-1)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reduced the search space for pairs from >1mm to approximately 2,000.\n",
    "\n",
    "### Cluster Visualization\n",
    "We have found 11 clusters. The data are clustered in 52 dimensions. As an attempt to visualize what has happened in 2d, we can try with [T-SNE](https://distill.pub/2016/misread-tsne/). T-SNE is an algorithm for visualizing very high dimension data in 2d, created in part by Geoff Hinton. We visualize the discovered pairs to help us gain confidence that the `DBSCAN` output is sensible; i.e., we want to see that T-SNE and DBSCAN both find our clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne = TSNE(learning_rate=1000, perplexity=25, random_state=1337).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, facecolor='white')\n",
    "plt.clf()\n",
    "plt.axis('off')\n",
    "\n",
    "plt.scatter(\n",
    "    X_tsne[(labels!=-1), 0],\n",
    "    X_tsne[(labels!=-1), 1],\n",
    "    s=100,\n",
    "    alpha=0.85,\n",
    "    c=labels[labels!=-1],\n",
    "    cmap=cm.Paired\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    X_tsne[(clustered_series_all==-1).values, 0],\n",
    "    X_tsne[(clustered_series_all==-1).values, 1],\n",
    "    s=100,\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "plt.title('T-SNE of all Stocks with DBSCAN Clusters Noted');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see how many stocks we found in each cluster and then visualize the normalized time series of the members of a handful of the smaller clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(\n",
    "    xrange(len(clustered_series.value_counts())),\n",
    "    clustered_series.value_counts()\n",
    ")\n",
    "plt.title('Cluster Member Counts')\n",
    "plt.xlabel('Stocks in Cluster')\n",
    "plt.ylabel('Cluster Number');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To again visualize if our clustering is doing anything sensible, let's look at a few clusters (for reproducibility, keep all random state and dates the same in this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of stocks in each cluster\n",
    "counts = clustered_series.value_counts()\n",
    "\n",
    "# let's visualize some clusters\n",
    "cluster_vis_list = list(counts[(counts<20) & (counts>1)].index)[::-1]\n",
    "\n",
    "# plot a handful of the smallest clusters\n",
    "for clust in cluster_vis_list[0:min(len(cluster_vis_list), 3)]:\n",
    "    tickers = list(clustered_series[clustered_series==clust].index)\n",
    "    means = np.log(pricing[tickers].mean())\n",
    "    data = np.log(pricing[tickers]).sub(means)\n",
    "    data.plot(title='Stock Time Series for Cluster %d' % clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might be interested to see how a cluster looks for a particular stock. Large bank stocks share similar strict regulatory oversight and are similarly economic and interest rate sensitive. We indeed see that our clustering has found a bank stock cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_cluster = clustered_series.loc[symbols('JPM')]\n",
    "clustered_series[clustered_series == which_cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = list(clustered_series[clustered_series==which_cluster].index)\n",
    "means = np.log(pricing[tickers].mean())\n",
    "data = np.log(pricing[tickers]).sub(means)\n",
    "data.plot(legend=False, title=\"Stock Time Series for Cluster %d\" % which_cluster);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have sensible clusters of common stocks, we can validate the cointegration relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cointegrated_pairs(data, significance=0.05):\n",
    "    # This function is from https://www.quantopian.com/lectures/introduction-to-pairs-trading\n",
    "    n = data.shape[1]\n",
    "    score_matrix = np.zeros((n, n))\n",
    "    pvalue_matrix = np.ones((n, n))\n",
    "    keys = data.keys()\n",
    "    pairs = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            S1 = data[keys[i]]\n",
    "            S2 = data[keys[j]]\n",
    "            result = coint(S1, S2)\n",
    "            score = result[0]\n",
    "            pvalue = result[1]\n",
    "            score_matrix[i, j] = score\n",
    "            pvalue_matrix[i, j] = pvalue\n",
    "            if pvalue < significance:\n",
    "                pairs.append((keys[i], keys[j]))\n",
    "    return score_matrix, pvalue_matrix, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dict = {}\n",
    "for i, which_clust in enumerate(ticker_count_reduced.index):\n",
    "    tickers = clustered_series[clustered_series == which_clust].index\n",
    "    score_matrix, pvalue_matrix, pairs = find_cointegrated_pairs(\n",
    "        pricing[tickers]\n",
    "    )\n",
    "    cluster_dict[which_clust] = {}\n",
    "    cluster_dict[which_clust]['score_matrix'] = score_matrix\n",
    "    cluster_dict[which_clust]['pvalue_matrix'] = pvalue_matrix\n",
    "    cluster_dict[which_clust]['pairs'] = pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for clust in cluster_dict.keys():\n",
    "    pairs.extend(cluster_dict[clust]['pairs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"We found %d pairs.\" % len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"In those pairs, there are %d unique tickers.\" % len(np.unique(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair Visualization\n",
    "Lastly, for the pairs we found and validated, let's visualize them in 2d space with T-SNE again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = np.unique(pairs)\n",
    "X_df = pd.DataFrame(index=returns.T.index, data=X)\n",
    "in_pairs_series = clustered_series.loc[stocks]\n",
    "stocks = list(np.unique(pairs))\n",
    "X_pairs = X_df.loc[stocks]\n",
    "\n",
    "X_tsne = TSNE(learning_rate=50, perplexity=3, random_state=1337).fit_transform(X_pairs)\n",
    "\n",
    "plt.figure(1, facecolor='white')\n",
    "plt.clf()\n",
    "plt.axis('off')\n",
    "for pair in pairs:\n",
    "    ticker1 = pair[0].symbol\n",
    "    loc1 = X_pairs.index.get_loc(pair[0])\n",
    "    x1, y1 = X_tsne[loc1, :]\n",
    "\n",
    "    ticker2 = pair[0].symbol\n",
    "    loc2 = X_pairs.index.get_loc(pair[1])\n",
    "    x2, y2 = X_tsne[loc2, :]\n",
    "      \n",
    "    plt.plot([x1, x2], [y1, y2], 'k-', alpha=0.3, c='gray');\n",
    "        \n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], s=220, alpha=0.9, c=[in_pairs_series.values], cmap=cm.Paired)\n",
    "plt.title('T-SNE Visualization of Validated Pairs');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have found a nice number of pairs to use in a pairs trading strategy. Note that the unique number of stocks is less than the number of pairs. This means that the same stock, e.g., AEP, is in more than one pair. This is fine, but we will need to take some special precautions in the **Portfolio Construction** stage to avoid excessive concentration in any one stock. Happy hunting for pairs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*This presentation is for informational purposes only and does not constitute an offer to sell, a solicitation to buy, or a recommendation for any security; nor does it constitute an offer to provide investment advisory or other services by Quantopian, Inc. (\"Quantopian\"). Nothing contained herein constitutes investment advice or offers any opinion with respect to the suitability of any security, and any views expressed herein should not be taken as advice to buy, sell, or hold any security or as an endorsement of any security or company.  In preparing the information contained herein, Quantopian, Inc. has not taken into account the investment needs, objectives, and financial circumstances of any particular investor. Any views expressed and data illustrated herein were prepared based upon information, believed to be reliable, available to Quantopian, Inc. at the time of publication. Quantopian makes no guarantees as to their accuracy or completeness. All information is subject to change and may quickly become unreliable for various reasons, including changes in market conditions or economic circumstances.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
